{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression, 2 flavors\n",
    "\n",
    "Prepared by Luna Lin (y_lin@g.harvard.edu)\n",
    "\n",
    "### 1. Optimization, single variable\n",
    "Consider a dataset\n",
    "$$ [x_1, y_1], [x_2, y_2], \\ldots, [x_N, y_N]. $$\n",
    "We want to have a model that can explain this dataset. The simplest is a linear model, $y = \\alpha x + \\beta$.\n",
    "Our aim is to minimize the sum of the squared error, i. e.\n",
    "$$\n",
    " E = \\sum_{i=1}^N \\left[y_i - (\\alpha x_i + \\beta)\\right]^2\n",
    "$$\n",
    "\n",
    "The sum of squares $E$ is the so-called convex functions and it's always non-negative, if we take the derivative of the sum $E$ and set it to zero, we will find its minimum.\n",
    "Therefore, take derivative with respective to $\\alpha$,\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial \\alpha} = \\sum_{i=1}^N -2 \\left[y_i - (\\alpha x_i + \\beta)\\right] x_i = 0\n",
    "    \\\\\n",
    "    \\implies \\sum_{i=1}^N x_i y_i - \\alpha \\sum_{i=1}^N x_i x_i + \\beta \\sum_{i=1}^N x_i = 0\n",
    "$$\n",
    "\n",
    "Take derivative with respective to $\\beta$,\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial \\beta} = \\sum_{i=1}^N -2 \\left[y_i - (\\alpha x_i + \\beta)\\right] = 0 \\\\\n",
    "    \\implies \\sum_{i=1}^N y_i - \\alpha \\sum_{i=1}^N x_i + \\beta N = 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimization, multiple variables\n",
    "\n",
    "Consider a dataset with indepedent variables $\\mathbf{x_i}$ being a vector, and dependent variable $\\mathbf{y_i}$ being a scalar, \n",
    "$$ [\\mathbf{x_1}, y_1], [\\mathbf{x_2}, y_2], \\ldots, [\\mathbf{x_N}, y_N].$$\n",
    "Also consider a set of parameters $\\mathbf{\\theta}$, a vector with the same length as $\\mathbf{x}$.\n",
    "A linear model that may describe this data is a matrix equation\n",
    "$$\n",
    "    \\mathbf{\\theta}^\\mathsf{T} \\mathbf{x} = y\n",
    "$$\n",
    "Now if we arrange our indepedent variable data in a \"tall\" matrix as\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix} \n",
    "\\mathbf{x_1} \\\\\n",
    "\\mathbf{x_2} \\\\\n",
    "\\ldots \\\\\n",
    "\\mathbf{x_N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and arrange all depdent variables in a vector $\\mathbf{y} = (y_1, y_2, \\ldots, y_N)^\\mathsf{T}$, we can write the sum of squared errors as\n",
    "$$\n",
    "E = \\left( \\mathbf{X}\\theta - \\mathbf{y} \\right)^\\mathsf{T} \\left( \\mathbf{X}\\theta - \\mathbf{y} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the expression for $E$ above we have\n",
    "$$ (\\theta^\\mathsf{T} \\mathbf{X}^\\mathsf{T} - \\mathbf{y}^\\mathsf{T}) \\left( \\mathbf{X}\\theta - \\mathbf{y} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\theta^\\mathsf{T} \\mathbf{X}^\\mathsf{T}\\mathbf{X} \\theta - \\theta ^\\mathsf{T}\\mathbf{X}^\\mathsf{T} \\mathbf{y} - \\mathbf{y}^\\mathsf{T} \\mathbf{X} \\theta - \\mathbf{y}^\\mathsf{T} \\mathbf{y} $$\n",
    "\n",
    "$$\n",
    " =\\theta^\\mathsf{T} \\mathbf{X}^\\mathsf{T}\\mathbf{X} \\theta -2 \\theta ^\\mathsf{T}\\mathbf{X}^\\mathsf{T} \\mathbf{y}- \\mathbf{y}^\\mathsf{T} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take derivative with respect to $\\theta$ and set it to zero, we have\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\theta} = 2 \\mathbf{X} ^\\mathsf{T} \\mathbf{X} \\theta - 2 \\mathbf{X} ^ \\mathsf{T} \\mathbf{y} = 0 \\implies \\mathbf{X} ^\\mathsf{T} \\mathbf{X} \\theta =  \\mathbf{X} ^ \\mathsf{T} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\mathbf{X}$ is sometimes called the desgin matrix, and the last equation above is called the normal equation.\n",
    "\n",
    "The normal equation can be solved by \n",
    "$$\n",
    "\\theta = (\\mathbf{X} ^\\mathsf{T} \\mathbf{X} )^{-1} \\mathbf{X} ^ \\mathsf{T} \\mathbf{y}\n",
    "$$\n",
    "Solving the normal equation can sometimes be tricky, because the matrix $\\mathbf{X}^\\mathsf{T} \\mathbf{X}$ can be ill conditioned. This means, with a small variation in $\\theta$, the matrix multiplication $\\mathbf{X}^\\mathsf{T} \\mathbf{X} \\theta$ can give wildly different answers. To handle these situation, we will need more advance linear algebra. In a nutshell, we will decompose $\\mathbf{X}$ using QR or SVD first, instead of solving the normal equation first. Often this is more computational efficient and numerical stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Maximum likelihood estimation (MLE)\n",
    "Another way to think about regression is via probability. Least square linear regression has a very nice correspondence with maximum likelihood estimation. Imagine each data point in the first example above as an independent observation, i.e. the error in the observation is an i. i. d. (indepedent and identically distributed) random variable, $\\epsilon = y_i - (\\alpha x_i + \\beta) \\sim \\mathcal{N} ( 0 , \\sigma^2)$.\n",
    "\n",
    "This means, we can write the probability of observing this data point as\n",
    "$$\n",
    "p_i(y_i | x_i; \\alpha, \\beta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n",
    "\\exp \\left( -\\frac{(y_i - (\\alpha x_i + \\beta))^2}{2\\sigma^2}\\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observing the set of data point is the joint probability, since they are all indepedent, the joint probability is just the product of each probability, i. e. \n",
    "$$\n",
    "P = \\prod_{i=1}^N p_i\n",
    "$$\n",
    "We want to maximize this probability, i.e. the likelihood that we observe this set of data. A trick is first take the log of $P$ (called the log-likelihood), which doesn't change where $P$ assume maximum or minimum, then take derivative with respect to the parameters. Taking the natural log,\n",
    "$$\n",
    "    \\log (P) = - \\frac12 N \\log(2\\pi) - N \\log(\\sigma) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N \\left[y_i - (\\alpha x_i + \\beta)\\right]^2 \n",
    "$$\n",
    "\n",
    "Notice the last term, except for the prefactor of $\\frac{1}{2\\sigma^2}$ is exactly the expression we started with in the first example. In MLE, we can also get an estimate for the variance $\\sigma^2$.\n",
    "For multiple variables, there is also a correspondence to MLE using multivariate Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exercise\n",
    "\n",
    "Given $y = \\sin (x) + 2 $, generate a fake dataset with Gaussian noise for 100 points in $x \\in [-0.5, 0.5]$, and run least squares regression in several ways. You might say $sin(x)$ is not a linear function, is it a good idea to fit a line to it anyway? In fact, $sin(x)$ looks quite linear near 0, the slope there is approximately 1.\n",
    "\n",
    "1. Use Numpy's random module, generate 100 random data points, i.e. at random positions within the range $[-0.5, 0.5]$ and with a Gaussian noise, $y = \\sin(x) + 2 +\\epsilon$, where $\\epsilon$ is drawn from a Gaussian distribution with mean 0, and standard deviation 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Notice that in example 2, there wasn't an explicit term to fit constant, this is because it is absorbed into the parameter set $\\theta$. If we let one of the indepedent variable be 1, i.e. $\\mathbf{x_i} = [x_i, 1]$, then the parameter set $\\theta = [\\theta_1, \\theta_2]$ will have $\\theta_1$ as coefficient for $x$, and $\\theta_2$ as the constant term. Construct the design matrix using the fake data set you generated above, and solve the normal equation with numpy functions such as numpy.transpose, numpy.dot, and numpy.linalg.inv. What is the result of the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After looking up numpy.linalg.lstqr, solve the least squares problem using that function. What is the result of the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and further reading\n",
    "1. why squared error: https://www.benkuhn.net/squared/\n",
    "2. more on connection to Maximum Likelihood Estimation https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
