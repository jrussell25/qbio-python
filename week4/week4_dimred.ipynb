{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae296a81-c3ff-450f-92f6-a5a83654ffe8",
   "metadata": {},
   "source": [
    "# Introduction to dimensionality reduction (with `scikit-learn`)\n",
    "Prepared by Jacob Zavatone-Veth (<jzavatoneveth@g.harvard.edu>)\n",
    "\n",
    "High-dimensional data is difficult to visualize, and our intuition for objects in more than three dimensions is poor. Yet, many high-dimensional datsets contain low-dimensional structure. These exercises will introduce several (mostly elementary) methods for [_dimensionality reduction_](https://en.wikipedia.org/wiki/Dimensionality_reduction), which seeks to uncover such structure. We'll use the [scikit-learn](https://scikit-learn.org) package, which provides convenient implementations of many common dimensionality reduction algorithms in a standardized form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7035e-a57c-45ac-90dc-6db93ce42e80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is (almost certainly) the most commonly-used method for dimensionality reduction. Given a set of $n$-dimensional datapoints $\\{\\mathbf{x}_{\\mu}\\}_{\\mu=1}^{p}$, PCA seeks an $m$-dimensional linear projection such that the empirical variance of the data in the low-dimensional subspace is maximized.\n",
    "\n",
    "PCA can be performed using the following intuitive (though inefficient) algorithm. Assuming that the data is centered (i.e., that the empirical mean $\\frac{1}{p} \\sum_{\\mu=1}^{p} \\mathbf{x}_{\\mu}$ is zero), the projection vector for the first principal component can be computed $$\\mathbf{w}_{1} = \\underset{\\Vert \\mathbf{w}_{k} \\Vert_{2} = 1}{\\arg\\max} \\sum_{\\mu=1}^{p} (\\mathbf{x}_{\\mu} \\cdot \\mathbf{w}_{1})^2$$ The remaining $m-1$ desired projection vectors can then be computed as $$\\mathbf{w}_{k} = \\underset{\\Vert \\mathbf{w}_{k} \\Vert_{2} = 1}{\\arg\\max} \\sum_{\\mu=1}^{p} (\\mathbf{x}_{\\mu}^{(k)} \\cdot \\mathbf{w}_{k})^2,$$ where $$\\mathbf{x}_{\\mu}^{(k)} = \\left(\\mathbf{I}_{n} - \\sum_{l=1}^{k-1}\\mathbf{w}_{l} \\mathbf{w}_{l}^{\\top}\\right) \\mathbf{x}_{\\mu}^{(k-1)}$$ This procedure yields the eigenvectors of the empirical covariance matrix $\\hat{\\mathbf{C}} = \\frac{1}{p} \\sum_{\\mu=1}^{p} \\mathbf{x}_{\\mu} \\mathbf{x}_{\\mu}^{\\top}$. With $\\mathbf{W} = [\\mathbf{w}_{1},\\ldots,\\mathbf{w}_{m}]$, the _scores_ of the datapoints are then given as $\\mathbf{t}_{\\mu}=\\mathbf{W}^{\\top} \\mathbf{x}_{\\mu}$. This is the desired $m$-dimensional representation of the data. \n",
    "\n",
    "We'll demonstrate this using the famous [_Iris_ flower datset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which consists of measurements of three releated species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac537e67-8e38-4294-854e-e096e31e12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn import datasets, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3085d01-0509-46c1-8238-bf7ac465b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2e352-529e-4fb2-96d0-dd2a87480592",
   "metadata": {},
   "source": [
    "Now we need to create a [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) object. We'll compute 3 principal components. \n",
    "\n",
    "To fit the transformation and obtain the scores, we can call the [fit_transform()](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform) method. Other sklearn dimensionality reduction methods operate similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3b0d21-26ce-48d2-befb-8b7c39dd58c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "392b0c84-642a-4bff-96c7-5b1609b96206",
   "metadata": {},
   "source": [
    "Now we can plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7ab1d-dc67-4e86-97bb-ba846fe5cc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d983bfd-e05f-4920-a61e-f2124ae0040b",
   "metadata": {},
   "source": [
    "## Exercise 1: What are some limitations of PCA? \n",
    "As an exercise, you'll now work through a simple example that demonstrates some limitations of PCA. We will use the [`circles`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html) dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00af1599-55c4-4e27-81c6-79dfbab7c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'circles' datset\n",
    "X, y = datasets.make_circles(n_samples=400, factor=.3, noise=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89496ddb-1afa-4c2a-abc3-8e1050bde301",
   "metadata": {},
   "source": [
    "**Part (a).** Since these data are two-dimensional, you should be able to plot them directly. Based on their structure, how do you think PCA will behave when applied to these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd964cb-1a54-46cf-ad72-03eced4d1672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6172e69c-1a41-4edf-8235-b6f2c6e9cd48",
   "metadata": {},
   "source": [
    "**Part (b).** Perform PCA on this dataset and plot the results. What do you observe? \n",
    "\n",
    "_Hint: How many components should you use?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ca618-e724-47d5-af08-367997e41e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6594bc8b-e662-4878-ae3c-d02c7a9b6960",
   "metadata": {},
   "source": [
    "**Part (c).** Now we'll try a non-linear dimensionality reduction method. Try running [kernel PCA](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis) using sklearn's `KernelPCA`. How do these results compare to PCA? Experiment with a few different kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c507121-fc6f-481e-8be1-c62a4ea21ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e1a800-f9da-4e53-bbfb-d8dfd51a95ba",
   "metadata": {},
   "source": [
    "## Exercise 2: the 'Swiss roll' dataset\n",
    "In this exercise, you'll experiment with a classic example of a high-dimensional dataset with low-dimensional structure: the so-called 'Swiss roll.' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76772867-4cf0-4c72-b0a2-c6e353da525c",
   "metadata": {},
   "source": [
    "**Part (a).** Load 1000 points from the datset.\n",
    "\n",
    "_Hint: use `datasets.make_swiss_roll`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0485d-ccc5-40e7-b310-be8f591e949d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76e88b14-db1b-47e5-8721-cc369b1ebeea",
   "metadata": {},
   "source": [
    "As the S-curve is embedded in three dimensions, you can plot the data directly. Color the scatter plot by the `t` coordinate returned by `make_swiss_roll`. What is the intrinsic dimension of the manifold? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef9a29-8316-49c5-be62-e9269b063029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf72fbde-5de2-4b1e-af2f-c47ee3321ba1",
   "metadata": {},
   "source": [
    "**Part (b).** Try reducing the dimensionality of the Swiss roll using PCA. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430027c-25ea-47f4-bfa6-8ad740289875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a4a11ec-eeab-49a3-bd22-c41162dcc152",
   "metadata": {},
   "source": [
    "**Part (c).** Experiment with a few of the non-linear manifold learning algorithms from [`sklearn.manifold`](https://scikit-learn.org/stable/modules/manifold.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6998b-582f-4cbf-9477-dcaec3d2ef09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
